# NN-from-scratch
Development of a neural network using only basic math operations

### 1. [Introduction](#introduction)
### 2. [Installation](#installation)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)

## Introduction
The project is a dense neural network with variable number of layers and layer sizes as well as its training and testing routines.
It was developed as a way to validate the knowledge aquired in the AI2 classes of THI University about how a dense neural network works in its lowest level.

## Math behing 

### 
$ŷ$ : Output activations vector (prediction)

$y$ : Label vector (how the output should be)

$x$ : Input vector

$\varphi(z)$ : Activation function

$L(ŷ, y)$ : Loss function

$w^{[1]}$ : Weight matrix for the layer l

$a^{[l]}$ : Activations matrix for the layer l

$b^{[1]}$ : Biases matrix for the layer l


### Forward pass:
$z^{[1]} = w^{[1]} \cdot a^{[l-1]} + b^{[1]}$

$a^{[1]} = \varphi (z^{[1]})$




## Algorithm and function

Forward function


## Training process
This is the contributing section.

